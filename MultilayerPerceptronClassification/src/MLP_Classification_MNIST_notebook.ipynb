{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MNIST Dataset Classification Task\n","## Charis Filis Academic-Id: 9449"]},{"cell_type":"markdown","metadata":{},"source":["### Import dependecies "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from __future__ import absolute_import, division, print_function\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras import regularizers\n","from tensorflow.keras import initializers\n","from keras.utils import np_utils\n","import keras_tuner as kt\n","import sympy\n","import numpy as np\n","import seaborn as sns\n","import collections\n","# visualization tools\n","%matplotlib inline\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'Input' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\harry\\Documents\\Computational_Intelligence_Thmmy\\Assignment\\MultilayerPerceptronClassification\\src\\MLP_Classification_MNIST_notebook.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harry/Documents/Computational_Intelligence_Thmmy/Assignment/MultilayerPerceptronClassification/src/MLP_Classification_MNIST_notebook.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Building a toy network Simple Perceptron\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/harry/Documents/Computational_Intelligence_Thmmy/Assignment/MultilayerPerceptronClassification/src/MLP_Classification_MNIST_notebook.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m Input(tf\u001b[39m.\u001b[39mfloat32,[\u001b[39mNone\u001b[39;00m,\u001b[39m784\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harry/Documents/Computational_Intelligence_Thmmy/Assignment/MultilayerPerceptronClassification/src/MLP_Classification_MNIST_notebook.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m W \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(tf\u001b[39m.\u001b[39mzeros[\u001b[39m10\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/harry/Documents/Computational_Intelligence_Thmmy/Assignment/MultilayerPerceptronClassification/src/MLP_Classification_MNIST_notebook.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m a \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(tf\u001b[39m.\u001b[39mzeros[\u001b[39m10\u001b[39m])\n","\u001b[1;31mNameError\u001b[0m: name 'Input' is not defined"]}],"source":["# Building a toy network Simple Perceptron\n","x = Input(tf.float32,[None,784])\n","W = tf.Variable(tf.zeros[10])\n","a = tf.Variable(tf.zeros[10])\n","pred = tf.matmul((x,W)) + b \n","pred= model(x,pred)\n","\n","\n","correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n","loss_f = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from random import shuffle\n","def get_images(batch_size, n_iterations):\n","    return [mnist.train.next_batch(batch_size=batch_size,shuffle=True) for dummy_index in range(n_iterations) ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from random import shuffle\n","def get_images(batch_size, n_iterations):\n","    return [mnist.train.next_batch(batch_size=batch_size,shuffle=True) for dummy_index in range(n_iterations) ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def minibatch_training(optimization_t, learning_r, train_set, minibatch_size):\n","    accuracy_buf = np.zeros(len(train_set))\n","    cross_entr_loss_buf = np.zeros(len(train_set))\n","    \n","    opt = optimization_t(learning_r)\n","    # Retrieve Trainable variables\n","    tvs = tf.trainable_variables()\n","    # Create Buffer for accumulating gradients/weights \n","    accum_weights = [tf.Variable(tv.initialized_value(),trainable=False)] for tv in tvs\n","    # Operation  to initialize accum_weights to zero\n","    zero_ops  = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n","    \n","    # Operation to compute gradients from one minibatch\n","    gvs = opt.com "]},{"cell_type":"markdown","metadata":{},"source":["### Data Preprocessing"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of original training examples: 60000\n","Number of original test examples: 10000\n","Training data shape: (60000, 28, 28, 1)\n","Training y train shape (60000,)\n","Testing data shape: (10000, 28, 28, 1)\n"]}],"source":["(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","# Rescale the images from [0,255] to the [0.0,1.0] range.\n","x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n","n_classes = 10;\n","n_features = 784; # features are the pixels of each image\n","print(\"Number of original training examples:\", len(x_train))\n","print(\"Number of original test examples:\", len(x_test))\n","print(\"Training data shape:\",x_train.shape)\n","print(\"Training y train shape\", y_train.shape)\n","print(\"Testing data shape:\",x_test.shape)\n","# Training parameters.\n","learning_rate = 0.1\n","training_steps = 2000\n","batch_size = 256\n","display_step = 100\n","# Network parameters.\n","n_hidden_1 = 128 # 1st layer number of neurons.\n","n_hidden_2 = 256 # 2nd layer number of neurons."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Proper minibatch Split\n","# Instantiate an optimizer.\n","optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n","# Instantiate a loss function.\n","loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# Prepare the training dataset.\n","batch_size = 64\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","x_train = np.reshape(x_train, (-1, 784))\n","x_test = np.reshape(x_test, (-1, 784))\n","\n","# Reserve 10,000 samples for validation.\n","x_val = x_train[-10000:]\n","y_val = y_train[-10000:]\n","x_train = x_train[:-10000]\n","y_train = y_train[:-10000]\n","\n","# Prepare the training dataset.\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","# Prepare the validation dataset.\n","val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n","val_dataset = val_dataset.batch(batch_size)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Part 1    Different MLP Architectures Comparison"]},{"cell_type":"markdown","metadata":{},"source":["# model_default : Default settings for Dense Sequential Model"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_8 (Dense)             (None, 128)               100480    \n","                                                                 \n"," dense_9 (Dense)             (None, 256)               33024     \n","                                                                 \n"," dense_10 (Dense)            (None, 10)                2570      \n","                                                                 \n","=================================================================\n","Total params: 136,074\n","Trainable params: 136,074\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model_default = keras.Sequential()\n","# Input Layer\n","# 1st hidden Layer\n","model_default.add(layers.Dense(n_hidden_1,input_shape=(n_features,), activation='relu'))\n","# 2nd hidden Layer\n","model_default.add(layers.Dense(n_hidden_2,activation='relu'))\n","# Output Layer\n","model_default.add(layers.Dense(10))\n","model_default.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def default_Model(Model):\n","    \n","    # Set Layers\n","    model_default = keras.Sequential()\n","    # Input Layer\n","    # 1st hidden Layer\n","    model_default.add(layers.Dense(n_hidden_1,input_shape=(n_features,), activation='relu'))\n","    # 2nd hidden Layer\n","    model_default.add(layers.Dense(n_hidden_2,activation='relu'))\n","    # Output Layer\n","    model_default.add(Dense(10, activation='softmax', kernel_initializer=initializer))\n","    model_defaultl.summary()\n","    model_default.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["#### * Define Cross Entropy Loss\n","#### * Define Accuracy Metric"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cross-entropy loss function\n","# With this implementation we dont need extra softmax layer\n","def cross_entropy_loss(x,y):\n","    # Convert labels to int64 for this function\n","    y = tf.case(y,tf.int64)\n","    # Apply Softmax to logits and compute cross-entropy \n","    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=x)\n","    # Average loss across the batch\n","    return tf.reduce_mean(loss)\n","\n","# Accuracy metric\n","def accuracy(y_pred,y_gt):\n","    # Predicted class is the index of highest score in prediction vector\n","    correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.cast(y_gt,tf.int64))\n","    # take mean accros correct_prediction vector\n","    return tf.reduce_mean(tf.cast(correct_prediction,tf.float32),axis=1)\n","\n","# Optimizer \n","optimizer = tf.optimizers.RMSprop(learning_rate=0.001,rhp=0.9,momentum='0', epsilon=1e-7,centered=False,name=\"RMSpropOptim\")\n","optimizer = tf.optimizers.SGD(learning_rate=///)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"40fcdd4a0e98f93efe88366db2abec70317af01b338acd99e2ac692059567cf3"}}},"nbformat":4,"nbformat_minor":2}
